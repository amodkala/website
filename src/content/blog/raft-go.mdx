---
title: "Approaches to race condition handling in Raft leader election"
date: 2024-01-04T12:00:00-05:00
description: "Comparing waitgroups to context cancellation"
tags:
    - "projects"
isDraft: false
---
import A from '../../components/ExternalLink.astro';

# {frontmatter.title}

*link to 
<A
    href="https://github.com/amodkala/raft"
    text="github repository"
/>* 

## The Problem

{
// This election race condition was occurring because 
// <A 
//     href="https://github.com/amodkala/raft/blob/c95c88085f70f8255208634dfdec34e990954a94/proto/raft.proto#L7"
//     text="RequestVote RPCs"
// /> were sent from a candidate to its peers in parallel using 
// <A
//     href="https://gobyexample.com/goroutines" 
//     text="goroutines" 
// />, which the `startElection` function did not wait to finish before determining the election's results. 
// The full code for the `startElection` function looked like this at the time:
}

The following code implements the candidate component of leader election as 
specified in the Raft paper.
1. Vote for yourself.
2. Send RequestVote RPCs to all peers in parallel with the relevant information.
3. If your candidacy has ended, ignore the response.
4. If the response indicates that you are behind, change state to "follower" and disregard response.
5. If the vote was granted, count it.
6. Check whether a majority was obtained.


```go
func (cm *CM) startElection() {
	cm.mu.Lock()
	electionTerm := cm.currentTerm
	cm.lastReset = time.Now()
	cm.votedFor = cm.self
	cm.mu.Unlock()

	votes := 1

	for id, peer := range cm.peers {

		go func(id int, peer proto.RaftClient) {

			var lastLogIndex, lastLogTerm int32

			cm.mu.Lock()
            lastLogIndex = int32(len(cm.log) - 1)
            lastLogTerm = cm.log[lastLogIndex].Term
			cm.mu.Unlock()

			req := &proto.RequestVoteRequest{
				Term:         electionTerm,
				CandidateId:  cm.self,
				LastLogIndex: lastLogIndex,
				LastLogTerm:  lastLogTerm,
			}

			res, err := peer.RequestVote(context.Background(), req)
            if err != nil {
                log.Printf("%v", err)
            }

            cm.mu.Lock()

            if cm.state != "candidate" {
                cm.mu.Unlock()
                return
            }

            if res.Term > electionTerm {
                cm.becomeFollower(res.Term)
                cm.mu.Unlock()
                return
            }

            if res.Term == electionTerm && res.VoteGranted {
                votes += 1
            }

            cm.mu.Unlock()
		}(id, peer)
	}

	if votes > len(cm.peers)/2 {
		cm.becomeLeader()
		return
	}

	cm.startElectionTimer()
}
```

{
// To the untrained eye, this may seem fine. However, those familiar with goroutines will note that the 
// function that spawns a goroutine will not wait for the goroutine to finish executing before returning.
}

It does, however, contain a pretty glaring bug: nearly instantly after sending our RPCs, the candidate evaluates
the win condition, loses the election (as no votes were received instantly aside from our own) and begins 
a new election.

This led to a state of deadlock where every cluster node would initiate futile elections constantly, and 
the cluster never resolved to any kind of steady state. In handling the error, I evaluated two different 
solutions which I'll show below, and discuss the pros and cons of both approaches.

## Solution 1: Wait Groups

My first thought was to modify the function so that every peer's response was accounted for before 
evaluating the result of the election. The `sync` package from Go's standard library includes a 
useful mechanism for waiting until goroutines finish, the 
<A
    text="wait group" 
    href="https://gobyexample.com/waitgroups" 
/>. 

Adding wait groups to the function was dead simple, in fact I only needed to add 4 lines of code (5 if 
you include the import statement)

```go
func (cm *CM) startElection() {
	cm.mu.Lock()
	electionTerm := cm.currentTerm
	cm.lastReset = time.Now()
	cm.votedFor = cm.self
	cm.mu.Unlock()

	votes := 1

    var wg sync.WaitGroup

	for id, peer := range cm.peers {

        wg.Add(1)

		go func(id int, peer proto.RaftClient) {
            defer wg.Done()

			var lastLogIndex, lastLogTerm int32

			cm.mu.Lock()
            lastLogIndex = int32(len(cm.log) - 1)
            lastLogTerm = cm.log[lastLogIndex].Term
			cm.mu.Unlock()

			req := &proto.RequestVoteRequest{
				Term:         electionTerm,
				CandidateId:  cm.self,
				LastLogIndex: lastLogIndex,
				LastLogTerm:  lastLogTerm,
			}

			res, err := peer.RequestVote(context.Background(), req)
            if err != nil {
                log.Printf("%v", err)
            }

            cm.mu.Lock()

            if cm.state != "candidate" {
                cm.mu.Unlock()
                return
            }

            if res.Term > electionTerm {
                cm.becomeFollower(res.Term)
                cm.mu.Unlock()
                return
            }

            if res.Term == electionTerm && res.VoteGranted {
                votes += 1
            }

            cm.mu.Unlock()
		}(id, peer)
	}

    wg.Wait()

	if votes > len(cm.peers)/2 {
		cm.becomeLeader()
		return
	}

	cm.startElectionTimer()
}
```

You can think of wait groups as counters, you increment for every parallel process you want to 
wait for (`wg.Add(1)`), and decrement to indicate that the process is finished (`wg.Done()`). 
The key line is `wg.Wait()`, which blocks function execution until all the pending processes 
tracked by the wait group are done.

{
// This solved the issue immediately, and I was even able to drop the election timers by a factor 
// of 10 without any breaking issues. I pushed this fix to GitHub and was ready to call it done, 
// until later that night as I was about to fall asleep I had a sudden realization: 
// fundamentally, **we don't care whether all votes are in.**
}

## Solution 2: Channels with Context Cancellation

The issue with the above approach is that as a candidate it doesn't matter how every one of our 
peers respond, we only care whether we have received a plurality of the cluster's votes. The way 
I went about implementing this behaviour was to again use two of the primary tools used by Go 
programmers: 
<A
    text="channels"
    href="https://gobyexample.com/channels"
/> and 
<A
    text="context"
    href="https://gobyexample.com/context"
/>.

In essence, the new code does the following:
1. Initialize a buffered channel of boolean vote responses (true or false).
2. Initialize a cancellable context, and automatically cancel it when the function stops 
executing.
3. Pass the context and channel to each goroutine; use the context for the RPC call, and 
send the response (yes or no) on the channel depending on whether the context has been 
cancelled or not.
4. Back in the election function listen for the expected number of responses, but if a 
majority is achieved then cancel the context, close the channel, and change state to 
leader.
5. If after the expected number of responses a majority isn't achieved and state is still 
candidate, start a new election.

```go
func (cm *CM) startElection() {
	cm.mu.Lock()
	electionTerm := cm.currentTerm
	cm.lastReset = time.Now()
	cm.votedFor = cm.self
	cm.mu.Unlock()

	votes := 1

    voteChan := make(chan bool, len(cm.peers))
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel() // Ensure cancellation at the end of the function

	for _, peer := range cm.peers {

		go func(ctx context.Context, peer proto.RaftClient, voteChan chan bool) {
            defer func() {
                // Close channel only when all goroutines are done
                if r := recover(); r != nil && r == "send on closed channel" {
                    log.Println("Channel was closed, vote result not sent")
                }
            }()

			var lastLogIndex, lastLogTerm int32

			cm.mu.Lock()
            lastLogIndex = int32(len(cm.log) - 1)
            lastLogTerm = cm.log[lastLogIndex].Term
			cm.mu.Unlock()

			req := &proto.RequestVoteRequest{
				Term:         electionTerm,
				CandidateId:  cm.self,
				LastLogIndex: lastLogIndex,
				LastLogTerm:  lastLogTerm,
			}

			res, err := peer.RequestVote(ctx, req)
            if err != nil {
                log.Printf("%v", err)
                voteChan <- false
                return
            }

            cm.mu.Lock()
            defer cm.mu.Unlock()

            if cm.state != "candidate" {
                return
            }

            if res.Term > electionTerm {
                cm.becomeFollower(res.Term)
                return
            }

            select {
            case voteChan <- res.Term == electionTerm && res.VoteGranted:
            case <-ctx.Done():
                log.Printf("Context cancelled, stopping vote request")
                return
            }
		}(ctx, peer, voteChan)
	}

    // Collect responses until the majority is reached or all votes are counted
    for i := 0; i < len(cm.peers); i++ {
        if vote := <-voteChan; vote {
            votes++
            if votes > len(cm.peers)/2 {
                cm.becomeLeader()
                cancel() // Cancel remaining goroutines
                break
            }
        }
    }

    // Clean up
    close(voteChan)

    if cm.state == "candidate" {
        go cm.startElectionTimer()
    }
}
```
The performance gains are negligible on my local test setup, however I am inclined to keep this 
solution for now as it is more "correct," and my roadmap for the project includes setting up a 
containerized and distributed demo where more latency is introduced and the effects could end up 
becoming more pronounced. 

{
// 
// The results here are a mixed bag. On the one hand this approach works well, yet 
// testing on a local network means that the performance gains relative to the wait 
// group approach are negligible at the cost of a great deal of added complexity. 
// 
// 
// 
// 
// - Section 6 (cluster membership changes)
// - Section 7 (log compaction)
// - A more feature-complete client example
// 
// I hope you learned a bit from this iterative problem-solving process, I know I 
// definitely did. If you have any questions or comments, feel free to contact me 
// through the information posted on my [about page](/)
}
