---
title: "Retrospective: Raft consensus implemented in Go"
date: 2024-01-04T12:00:00-05:00
description: "Updating an older project of mine"
tags:
    - "projects"
---
import ExternalLink from '../../components/ExternalLink.astro';

# {frontmatter.title}

## Introduction

During the final internship recruiting season of my undergrad, the project that got the most 
attention from interviewers was my implementation of the 
<ExternalLink
    text="Raft"
    href="https://raft.github.io/raft.pdf"
/> consensus algorithm using Go and gRPC. In the ~2 years since then I haven't really touched 
the project, but now that I'm looking for full-time work I thought I'd see if I could make it 
even better with the benefit of hindsight and experience.

When I first revisited the project, my most pressing concerns were:
- [x] Solve race condition in the election process
- [x] Create local demo with log replication

The election race condition was occurring because RequestVote RPCs were sent from a candidate 
to its peers in parallel using 
<ExternalLink 
    href="https://gobyexample.com/goroutines" 
    text="goroutines" 
/> but the result of the election was determined immediately thereafter, introducing the 
possibility (and in this case, reality) that responses which took too long to receive were not 
being counted. To address the issue, I added a 
<ExternalLink 
    text="wait group" 
    href="https://gobyexample.com/waitgroups" 
/> mechanism to ensure that all responses were accounted for before declaring an election won 
or lost.

Creating a local demonstration was also fairly straightforward. Below, you'll see that the 
demonstration accepts a series of IP addresses where the first is for the current consensus 
module and the following addresses are for its peers. I then initialize a new consensus module 
and channel for reading back committed entries before starting the consensus module in its own 
goroutine. It is also here where I pass in the cluster configuration using the 
<ExternalLink 
    text="functional options" 
    href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis" 
/> pattern. 

With the consensus module started in the background, I wait a generous 4 seconds 
to allow leader election to occur before sending a replicate command to all consensus modules. 
Of course, only the currently elected leader accepts the request, but as expected every client 
receives the committed entries through the commit channel, which the demo listens for continuously.

```go
package main

import (
    "log"
    "os"
    "time"

    "github.com/amodkala/raft"
)

func main() {
    addr := os.Args[1]
    peers := os.Args[2:]
    cm, commitChan := raft.New(addr)

    go cm.Start(raft.WithLocalPeers(peers))

    time.Sleep(4 * time.Second)
    cm.Replicate([]byte("hello"), []byte("world"))

    for {
        entry := <-commitChan
        log.Printf("client of %s got entry %s", addr, string(entry))
    }
}
```

With my top priorities out of the way, I now have a stable local testing environment to add features, 
which could include:
- Section 6 (cluster membership changes)
- Section 7 (log compaction)
