Designing Data-Intensive Applications  
*Martin Kleppmann*

## Chapter 1

### Intro

Data-intensive applications are those where performance bottlenecks are
attributable to properties of the data involved (amount, complexity, changes)
rather than limits of the underlying hardware performance.

They are often comprised of many parts with different responsibilities,
including
- databases: store data
- caches: store results of expensive operations
- indexes: search and/or filter data
- stream processing: asynchronous message transportation
- batch processing: periodically process large amounts of data

Three of our main concerns when designing such systems are
- reliability: system should function correctly under adverse conditions
- scalability: growth should be easily manageable
- maintainability: the system should be easy to work with

### Reliability

Reliability is about the system functioning according to expectations even when
encountering faults. A system which is built to withstand faults is called 
fault-tolerant. 

Note that we do not consider faults to equal failures. The distinction is that 
faults represent a deviation from expected behaviour, where failures are a 
cessation of the functionality in question.

Hardware faults were the primary concern for developers when applications ran
on single servers, however the shift to systems that span multiple machines
has resulted in the need for software-based fault-tolerance techniques.

### Scalability

Scalability is the system's ability to handle load, and designing systems with
scalability in mind involves addressing hotspots. Consider the "post" and 
"view timeline" operations for Twitter. "Post" is a write operation of sorts,
and "view timeline" is its corresponding read operation. 
Two possible approaches to designing this system are
- writing tweets to a database, then using a relational operation to retrieve
all tweets by a user's following when they send a read request.
- writing a tweet to a cache per user, then simply retrieving the cache when 
a user tries to read their timeline.

The former approach is read-heavy while the latter is write-heavy, but
examining usage patterns we see that there are around 100x more reads
than writes, so it makes sense to favour the write-heavy approach if timely
delivery is of concern.

For async services, we care about throughput, or how many records we can 
process in a given span of time. For interactive applications, response time,
the amount of time from client sending a request to receiving a response, is 
critical.

Response times are usually discussed in terms of percentiles, where the 95th
percentile (p95) is the response time for which 95% of responses take less 
time. Response times can be affected by circumstances outside of the scope of
the request, for example if an operation is holding up a queue. In this case,
the server would report the operation completing quickly, but the client would
have experienced a delay, so it is important to measure response times 
client-side. In cases where clients depend on responses from parallel requests,
the probability that at least one is slow grows with the number of calls, also
referred to as tail latency amplification.

Approaches to scalability fall under two categories
- horizontal scaling: distributing load across machines
- vertical scaling: adding compute to the machine

How scale is handled is largely application-dependent, with request volume and
access patterns being just two of the factors to take into consideration.

### Maintainability

Maintainability is about employing practices that extend the lifespan of the 
system. These practices generally adhere to these principles
- operability: system should be easy for other teams to ensure smooth operation
- simplicity: should be easily understandable for new engineers
- evolvability: should facilitate adaptation in the future

## Chapter 2

### Relational Model vs Document Model

The relational model (most commonly SQL) organizes data into tables comprised
of rows. A common critique of this model is that mapping relational data to 
objects (as in object-oriented programming languages) requires the use of a
translation layer. Impedence mismatch is addressed by document-oriented 
databases, which allow you to store data in a common format like JSON.

Document-oriented databases make it easier to represent one-to-many 
relationships in data (e.g one LinkedIn user could have multiple job 
positions), but relational databases are better suited to handling many-to-one
or many-to-many relationships. These tend to arise when you normalize the 
database by representing frequently-duplicated information using IDs.

## Chapter 3

### Database Data Structures

Logs are append-only data structures used to record the operations that a 
database has processed. To avoid running out of disk space, logs can be 
segmented into files of a set size, (e.g once a log file hits a 4MB size limit, 
start writing to the next file). Since logs are append-only, they store all the
historical updates of a given record which can take up disk space 
unnecessarily. Segmentation allows us to use compaction, which is when we 
discard all duplicate keys in a log except for the most recent update. Just as
segmentation buys us compaction, compaction buys us mergeability.

Indexes are another type of data structure which facilitate data lookups. Index
data structures can make reading from a database more efficient, at the expense
of some performance overhead when writing data. One example is the hash index, 
where we use a hash map to store the byte offset of a given record, allowing 
for efficient lookups.

Sorted String Tables (SSTables) are similar to a standard append-only log, with
the caveat that records are sorted by key, and each key only appears once per 
segment (already achieved through compaction). Having sorted records means that
merging is more efficient, lookups can be accomplished using sparse indexes, 
and records can be grouped into compressable blocks.

Log-Structured Merge-Trees (LSM-Trees) extend the SSTable into a fully-fledged
storage engine using the following algorithm:
- write requests are added to an in-memory balanced tree data structure (also 
called the memtable)
- To facilitate crash-recovery, a temporary on-disk log can mirror the current
memtable, and it can be discarded once the memtable is flushed to disk.
- When the memtable hits a size threshold, write it to disk as an SSTable file
(easy since keys are sorted already)
- Read requests go through the memtable first, then to the SSTables in reverse-
chronological order
- Merging and compaction can be performed in the background.

Bloom filters are memory-efficient data structures which can be used to avoid
performing look-ups for a key that doesn't exist.

B-Trees are tree data structures where each node is a page which is made up of
either keys (leaf pages) or references to other pages. As write operations 
sometimes involve writing to disk multiple times, crash-resiliency is also 
handled by a write-ahead log (WAL) which must be appended to before any 
operation can be applied.

Compared to B-Trees, LSM-Trees face less write-amplification as a result of the
use of the in-memory memtable. LSM-Trees are also the winner when it comes to 
disk usage, as they are not limited by page sizes and thus can be compressed
more efficiently. B-Trees, however, do not face the performance overhead that 
the compaction process causes, and the fact that keys only exist in one place
means that concurrency primitives are more easily implementable.

### Transaction Processing or Analytics

Online transaction processing (OLTP) is the access pattern by which database 
operations are performed in response to user input, usually only interacting
with a small number of records using an index.

Online analytic processing (OLAP) is the access pattern for databases which 
involves scanning over a large number of records, usually making aggregate 
calculations

Data warehousing is a technique to prevent expensive OLAP operations from being
run directly on OLTP databases, which might affect the performance of the often
latency-sensitive databases. Data is extracted periodically from OLTP 
databases, transformed into schemas that facilitate analysis, and loaded into 
the data warehouse where OLAP operations can be performed on it without 
affecting production system performance. One data model, or schema, used in 
data warehouses is the star schema. In this schema, data warehouses consist of
fact tables which contain records that map to events, and whose columns often
contain foreign key references to dimension tables which have information about
the event.

### Column-Oriented Storage

Column-oriented storage is a paradigm wherein information isn't grouped by the 
record it is associated with (as in row-oriented storage), but rather by its 
type. Instead of accessing the 23rd record and reading information from just 
three columns, you only have to access the 23rd item in three columns. 

Bitmap-indexing is a compression technique that lends itself well to 
column-oriented storage schemes where the number of distinct values is much 
less than the number of items in the column. For a column of length n, the
bitmap for a specific value x is an array of length n consisting of 0s and 1s,
where a 1 at position p means that column\[p\] = x. Encoding columns this way
allows us to use bitwise operators to filter columns by value in a more 
efficient manner. Some techniques that enable this increased efficiency include
loading chunks of columns into L1 CPU cache and using single-instruction
multiple-data (SIMD) instructions.

Sorting column-stores is slightly complicated by the fact that we must preserve
order across all columns. To accomplish this while also making lookups more
efficient, we select one or more columns to be sorting keys. The idea is that
data is first sorted by the first sort key, then data with the same value for
the first sort key is sorted by the second sort key, and so on.

When writing to column stores, an update in-place method like B-Trees is not
feasible when also using some of the compression techniques mentioned above. We
would instead opt to use an LSM-Tree approach

Data hypercubes are used to cache the result of aggregate operations on data.
Consider a table with three different attributes -- date, product SKU, and
customer ID. Then a "net_price" cube would contain at each intersection of these
attributes the value of all purchases of a specific product by a specific
customer on a specific day. Then the edges of the data hypercube would be
projections of the aggregate data, controlling for an attribute. For example, we
could store the aggregate value of purchases of all products across all
customers on a specific date on one edge, or all historical purchases of all
products by specific customers on another.

## Chapter 4

### Formats for Encoding Data

Programs interact with data in one of two forms, either using pointers to data
structures (manipulable and provide efficient access) or encoded as a byte
sequence for disk writing or network transportation. We transform data between
these two representations using serialization/deserialization, also referred to
as marshalling/unmarshalling.

Standardized, language-agnostic formats for representing data can be
human-readable, or "textual", as is the case with JSON, XML, and CSV. These have
the advantage of easy interoperability with third-party libraries and consumers.
On the other hand, binary encoding can be more compact and easier to parse.

Protocol Buffers is one such binary encoding library. It requires that users
define a schema for data similar to a struct definition, including a tag (key)
and type for each field. The serialized representation encodes this information
plus the length and value of data to compact the information.

Schema evolution in Protocol Buffers is facilitated by field tags, where new
fields are given a unique tag. Old code uses the encoded data length to skip
over fields that it isn't programmed to handle, ensuring forwards compatibility.
Backwards compatibility also depends on field tags, though new fields must be
optional or have a default value, lest they fail checks.

### Modes of Dataflow

Database writes involve encoding data to the specified format, while reads
involve decoding data. Data can be read or written by one or more processes, 
simultaneously or with years in between. Forwards and backwards compatibility
are clearly important here, but care must also be taken to not lose data in
between encoding and decoding operations.

Services are the processes on either end of a client-server connection where
data is transmitted between them over a network. Seperating business logic into
services allows for easier maintainability and coordination, in theory
increasing development velocity. Web services are those which communicate via
HTTP, with REST being the most common paradigm for designing web services.
Remote procedure call (RPC) frameworks are alternatives to REST that conceal
network calls in code, making them look like any other function call. RPC
frameworks often make use of binary encoding (e.g gRPC uses Protobuf) while REST
most often uses JSON or XML data formats.

Asynchronous message-passing systems can be thought of as a middle ground 
between the service and database paradigms. Like services, these systems rely on
network connections to pass data, however this data isn't directly sent from
service to service, instead it is stored temporarily by a message broker. This
model confers advantages such as buffering messages if the recipient is
unavailable, sending messages to multiple recipients at once, and decoupling
sender from receiver. This last point implies that senders no longer expect to
receive synchronous replies to their messages. We say that senders publish
messages to topics, which then go out to consumers of said topics.

## Chapter 7

### ACID

ACID is a set of safety guarantees that transactions seek to provide. It stands 
for 
- Atomicity 
- Consistency
- Isolation
- Durability

Atomicity (in the context of ACID) is the fault-tolerance property that for
writes grouped into one transaction, if one or more writes fail for any reason,
the entire transaction is aborted, and successful writes are rolled back.

Consistency (in ACID) is really a property of the application logic more so than
of the database -- it is the guarantee that any invariants of the data are
maintained for the duration of and following a transaction.

Isolation, better known as serializability, is the guarantee that concurrent
transactions behave as if they occurred in sequence, to ensure that operations
are applied correctly. If two transactions are applied concurrently, then one
will either see all updates made by other or none.

Durability guarantees that the data written by a succesful transaction will not
be lost unintentionally. Persisting data to disk, maintaining a write-ahead log,
and replicating data across multiple nodes are all examples of strategies that
provide durability.

### Weak Isolation Levels

Read committed is a strategy that guarantees no dirty reads and no dirty
writes. No dirty reads means that until a transaction is considered
"committed" no concurrent transactions will see the modifications made. No dirty
writes is a similar property where the result of applying concurrent
transactions should not be a combination of operations from each. One way read
committed is implemented involves concurrent locks at the object level, however
long-running transactions (like writes) may prevent other transactions from
acquiring the lock for their duration, potentially leading to cascading
slowdowns in the application. Databases may circumvent this issue by holding
both the old and new values and returning the old value to read operations for
the duration of a write operation.

Snapshot isolation is the strategy by which read ops don't block write ops, and
write ops don't block read ops. This is accomplished by maintaining multiple
versions of data at the same time, leading to the term multiple-version
concurrency control (MVCC)
